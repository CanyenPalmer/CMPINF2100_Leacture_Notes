{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af55ef09-4785-4499-8790-7e47f16d8522",
   "metadata": {},
   "source": [
    "## CMPINF 2100 Week 11 | Logistic - What is the Logit?\n",
    "\n",
    "This discussion will cover the summary of the slide show presentation from week 11's presentation.\n",
    "\n",
    "### Binary Classification\n",
    "- Binary classifiers CLASSIFY the **EVENT** or the **NON-EVENT**\n",
    "- The **EVENT** is commonly referred to as y=1\n",
    "- The **NON-EVENT** is commonly referred to as y=0\n",
    "- The Binary OUTPUT, y, is therefore an INTEGER data type!\n",
    "\n",
    "- However, this is NOT a regression problem!\n",
    "- The OUTPUT is a NUMBER but there are ONLY 2 unique values!\n",
    "- Regression is appropriate when the OUTPUT has many ALLOWABLE numeric values!\n",
    "\n",
    "### Binary Classification **CANNOT** work with LINEAR MODELS!\n",
    "- Remember the ASSUMPTIONS of the LINEAR MODEL!\n",
    "- The OUTPUT is Normally distributed around the AVERAGE OUTPUT (trend)!\n",
    "- Normally or Gaussian Bell-Curve is the important thing here\n",
    "\n",
    "#### Example of the Bell-Curve in use\n",
    "Consider the case with a SINGLE input lineraly related to the AVERAGE OUTPUT:\n",
    "- The OUTPUT is BINARY\n",
    "- It has 2 and ONLY 2 unique values!\n",
    "- The OUPUT is ONLY y=0 OR y=1. The AVERAGE CANNOT be at y=0 because the model CANNOT predict LESS THAN 0!\n",
    "\n",
    "This is illustrated when we place the Gaussian Distribution, centered at the value of `y=0` or `y=1`\n",
    "\n",
    "### Binary Classification - Therefore does NOT use a Gaussian!\n",
    "- The OUTPUT is NOT Normally distributed the AVERAGE OUTPUT!\n",
    "- Instead, a different probability distribution is used!\n",
    "- The OUPUT is **Bernoulli** distributed around the AVERAGE OUTPUT!\n",
    "\n",
    "- The AVERAGE OUTPUT or TREND has a special meaning!\n",
    "- The AVERAGE OUTPUT is the **EVENT PROBABILITY**!\n",
    "- Important: Binary Classification Models therefore **PREDICT THE EVENT PROBABILITY**\n",
    "\n",
    "#### Why does this matter?\n",
    "\n",
    "- Probabilities are BOUNDED between 0 and 1:\n",
    "    - The EVENT PROBABILITY cannot be NEGATIVE.\n",
    "    - The EVENT PROBABILITY cannot be GREATER THAN 1.\n",
    "This impacts how we model Binary Classification problems!\n",
    "\n",
    "Note the equation for the LINEAR PROBABILITY MODEL:\n",
    "$\\mu = \\beta_0 + \\beta_1 \\times x$\n",
    "\n",
    "- This equation allows for NEGATIVE TREND values!\n",
    "- This equation allows for TRENDS greater than 1!\n",
    "- Theres NOTHING in the equation itself that KEEPS the TREND within the BOUNDS of a probability!\n",
    "\n",
    "This is fundamentally wrong and we cannot use this! (although it is published in a lot of journals/articles)\n",
    "\n",
    "### Log Odds Ratio:\n",
    "\n",
    "- Therefore, we CANNOT directly model the EVENT PROBABILITY!\n",
    "- Instead, we must apply a TRANSFORMATION to the AVERAGE OUTPUT.\n",
    "- A popular approach is to MODEL or REGRESS the LOG ODDS RATIO.\n",
    "\n",
    "Formula: $logodds = \\beta_0 + \\beta_1 \\times x$\n",
    "\n",
    "- logodds ratio = our **slope** plus **our intercept** times the **input**\n",
    "\n",
    "#### What is the Log Odds Ratio?\n",
    "\n",
    "- The Log Odds Ratio is the NATURAL LOGARITHM of the ODDS RATIO (OR).\n",
    "- The OR defined as the PROBABILITY divided by 1 minus the PROBABILITY:\n",
    "\n",
    "$OR = Probability \\div (1 - Probability$)\n",
    "\n",
    "- We defined the EVENT PROBABILITY as 'mew', this the Odds-Ratio for the Binary classification problem is :\n",
    "$OR = \\mu \\div (1 - \\mu$)\n",
    "\n",
    "- The LOG ODDS RATIO is the NATURAL LOG of the OR:\n",
    "\n",
    "$log(OR) = log(\\mu \\div (1 - \\mu$))\n",
    "\n",
    "\n",
    "### The 'best-fit line' is applied to the LOG ODDS RATIO!\n",
    "\n",
    "- We model the Log-odds ratio instead of directly modeling the EVENT PROB\n",
    "\n",
    "$log(OR) = log(\\mu \\div (1 - \\mu)) = \\beta_0 + \\beta_1 \\times x$\n",
    "\n",
    "The LOG-ODDS is also known as the **LOGIT**. We are regressing the LOGIT of the EVENT PROBABILITY\n",
    "\n",
    "- We model the Log-oods ratio instead of directly modeling the EVENT PROBABILITY!\n",
    "\n",
    "$log(OR) = log(\\mu \\div (1 - \\mu)) = logit(\\mu) = \\beta_0 + \\beta_1 \\times x$\n",
    "\n",
    "We are regressing the LOGIT of the EVENT PROBABILITY -> GENERALIZING\n",
    "\n",
    "- The INVERSE LOGIT is known as the LOGISTIC!\n",
    "Logistic regression gets its name from the INVERSE of the LOGIT!\n",
    "\n",
    "$\\mu = INVERSE(logit( \\beta_0 + \\beta_1 \\times x)) = logit ^ -1 (\\beta_0 + \\beta_1 \\times x)$\n",
    "\n",
    "$\\mu$ is Avg Output\n",
    "\n",
    "### Important Asusumptions to remember with LOGISTIC REGRESSION\n",
    "\n",
    "- Although REGRESSION is in the name, this is NOT a regression method for continous outputs!\n",
    "    - Logistic regression is for BINARY CLASSIFICATION!\n",
    "- The model still predicts the AVERAGE OUTPUT, but the AVERAGE corresponsds to the EVENT PROBABILITY!\n",
    "- The LOGIT or LOG ODDS RATIO transformation is applied to make sure the EVENT PROBABILITY is bounded between 0 and 1!\n",
    "- The transformation allows us to use essentially everything else from the LINEAR MODEL!\n",
    "    - Logistic regression is a type of Generalized Linear Model (GLM)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048c51b6-58d6-4f96-80c9-1c311392a92f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
